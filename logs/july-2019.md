# 07/22/19

- \[1:05\]: Finished (almost) all of Chapter 1 of Raginsky notes.
- \[3:25\]: Thought about calculation for model. Identified a few special cases that need to be captured first; will simplify model based on this. Had some thoughts on how to make calculations more general, but didn't actually do much. Got stuck on capturing tensor decomposition need to get back to clearly formulating tasks.
- \[0:12\]: Watched part of video on mirror descent. Plan to watch full series in future.

Was quite ineffective today; need to be much more focused. A single high quality hour would have been more valuable than the entire 3:25 spent above. Most important thing to work on is **quality** of work. 

# 07/21/19

- \[0:33\]: Skimmed a paper on domain adaptation. Need more background in learning theory to understand what is going on, so decided to study Raginsky's lecture notes; hopefully, turns out to be a good long term investment of time.
- \[0:46\]: Handwrote notes on part of Chapter 1 of Raginsky's lecture notes.
- \[3:11\]: Typed up low-degree method notes. Finished planted clique and spiked tensor model minus the combinatorics. Started reading Bandeira et al.; calculations are involved but might be useful in the future. Need to finish spiked tensor model, read Bandeira et al., and begin calculation.
- \[0:53\]: Thought about speeding up rounding algorithm. Method of conditional expectations seems promising; need to read and try (or, quickly conclude speeding up not possible and move on).


# 07/20/19

- \[1:00\]: Wrote down some interesting thoughts related to [transfer learning](https://arxiv.org/pdf/1803.01833.pdf). New approach seems more tractable to me. Looked through work of Sanjoy Dasgupta, Samory Kpotufe and collaborators; possibly relevant, more digging needed.
- \[2:00\]: Typed up notes on low-degree method. Reviewed examples of planted clique and spiked tensor model; calculations make much more sense now. Should be able to make calculations generic to any PCA type model. Next step is to type up and understand calculations for Wishart model.
- \[1:20\]: Watched part of video on volume sampling, wasn't that useful. Watched video on PSD matrix approximation, was slightly useful.
- \[0:50\]: Thought about speeding up our algorithm to linear time, given yesterday's insight. Was a bit disorganized, but seems like several new opportunities have opened up. Need to quickly formalize questions and then understand literature. Problem should be reducible to matrix approximation by sampling, which is very well studied. Found paper on algorithmic problem of sampling multivariate Gaussians, uses Lanczos method.

Woke up 45 minutes late. Need to make sure not to fall into bad habit of using talk videos as an opportunity to avoid work.

# 07/19/19

- \[0:10\]: Handled some bureaucracy.
- \[1:19\]: Wrote up model and concrete task. Next step is to review low degreee LR method and begin calculation.
- \[1:30\]: Watched video of a talk by Tengyu Ma. Paper is relevant, but need to read it more carefully.
- \[0:15\]: Watched part of James Lee talk. Wasn't so useful.
- \[2:00\]: Started to look into leverage scores, soon realized direct application won't work. Realized a "smoother" version is needed: ridge leverage scores seem to be the right notion, will return to later. Then suddenly figured out how to fix our result, using an unrelated idea. Still need to write down and check.

Woke up 30 minutes late. Lessons:

1. Power of sleep.
2. The random idea that fixed our result had been floating around in head for a few days, but was too lazy to investigate. Could have saved a few days work by acting immediately.
3. [Professionals](https://www.youtube.com/watch?v=9Bnl6jMwGfQ) are smooth; they play at their own pace. Every single action is calculated and has a purpose. No wasted effort.
4. Developing a bad habit: In anticipation of finishing work at 3pm, productivity slips around 2:40pm and hesitancy to begin a new "hard" task becomes overpowering.

# 07/18/19

- \[1:00\]: Read Sections 11.1-11.2 of O'Donnell book.
- \[1:41\]: Thought about spectrally sparsifying our matrix.  Need to look for a correct notion of sparsification. Also thought about using leverage scores; it seems promising. Read part of Yin Tat's notes, started to make sense. Still need to complete his notes, try out the calculation for FHP, and return to inverse maintenance paper.
- \[2:43\]: Thought about modelling computation, clarified what needs to be done and drew connection to learning theory. Wrote down some thoughts. Reviewed circuits in Arora-Barak, handwrite some notes. Didn't get around to writing down full model; need to do that first thing tomorrow.


Fewer hours today due to delay in morning. Haven't been so effective at learning new tools; learning is going very slow. Maybe allotting larger time blocks purely dedicated to learning one topic, instead of a few small time blocks?

# 07/17/19

- \[1:15\]: Prepared for meeting.
- \[1:20\]: Meeting.
- \[1:35\]: Formalized some thoughts from meeting. Still need to write up summary and then begin calculations right away.
- \[1:11\]: Tried to learn about leverage scores; watched Petros Drineas talk but it wasn't useful. Next step is to just directly read Yin Tat lecture notes. Would have been done already starting with the lecture notes.
- \[0:17\]: Skimmed chapter on circuits in Arora-Barak.

Technical tools be learnt: Circuit complexity (Arora-Barak book), Leverage scores (Yin Tat lecture notes), Analysis of Gaussian functions (O'Donnell book Chapter 11), Low degree likelihood ratio (Hopkins thesis, Bandeira et al. paper)

# 07/16/19

- \[2:00\]: Looked into inverse maintenance paper. Need to understand $\sigma$ stability condition, so need to read up on leverage scores. Seems like I could have done all this much quicker.
- \[1:20\]: Finished typing notes on Banks et al. Next step is to formalize questions and future directions.
- \[0:15\]: Process, see below.
- \[2:00\]: Formalized two thoughts. Need to read Bandeira et al. 2019 paper and finish typing up all open problems.

Since last meeting, put in large number of hours, but don't have much to show. Moving forward, should only learn new material when confident of its direct relevance; remaining time is best spent directly on research. The right approach seems to be as follows:

1. Quickly formalize concrete technical questions.
2. Try to attack those questions with existing tools.
    - If successful, return to first step.
    - If not, understand why. Then, go learn new tools or build new ones. Return to second step.

I seem to be spending too much time on the second part of step 2 and not enough on step 1 or the first part of step 2.
# 07/15/19

- \[2:10\]: Revisited the dynamic eigenvector approach. Concluded that we need to exploit special properties of update matrix $D$. Next step is see if Lee-Sidford inverse maintenance paper has any relevant ideas.
- \[3:15\]: Planned out next steps and typed up handwritten notes on Banks et al. paper. Need to finish typing up sketch of calculations for two examples and formalize questions and intuitions. Once that is done, can finally begin to move forward.
- \[0:10\]: Looked at notes on canonical SDP relaxation for CSPs from Gupta-O'Donnell lecture notes since the approach might be useful. See also Chapter in O'Donnell book for more background.

# 07/14/19

- \[1:30\]: Skimmed \[Lee-Padmanabhan 2019\] and \[Arora-Kale 2007\] to see if solvers can be implemented without explicitly computing $A^T A$. Realized that probably won't work for our application because they use JL. Next step is to look into inverse maintenance approach.
- \[0:30\]: Read advisor's email and skimmed tensor network stuff.
- \[0:30\]: Setup logging system.

Main accomplishment was watching Djokovic-Federer Wimbledon final.
