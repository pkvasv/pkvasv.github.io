# August 2019

> gutta cavat lapidem non vi sed saepe cadendo

## 8/22/19

- \[\]:

## 8/21/19

- \[2:00\]: Watched video of talk on phase transitions in low-rank estimation.
- \[1:45\]: Extracted intuition for AMP and state evolution from [Bayati-Montanari 2011] and wrote notes. Returned to AMP for spiked matrix model.
- \[1:15\]: Watched parts of videos of talk on community detection and non-backtracking walk operator. Didn't learn too much, but will return in future.  

## 8/20/19

- \[1:24\]: Wrote notes for smooth convex optimization.
- \[0:15\]: Continued notes on spiked matrix models.
- \[1:38\]: Watched video of COLT 2017 plenary talk. Lots of interesting open problems, need to return in near future.
- \[1:45\]: Extracted intuition for BP for compressed sensing from [Donoho-Maleki-Montanari 2010].


## 8/19/19

- \[1:00\]: Read notes for smooth convex optimization.
- \[1:20\]: Developed intuition for AMP and state evolution.
- \[1:25\]: Watched video of lecture on optimization of SK Hamiltonian.
- \[0:20\]: Continued notes on spiked matrix models.
- \[1:15\]: Watched [video](https://www.birs.ca/events/2013/5-day-workshops/13w5136/videos/watch/201302260905-Gamarnik.html) of lecture on local algorithms.

## 8/18/19

- \[1:35\]: Finished up smooth non-convex optimization.
- \[0:15\]: Read introduction of paper on convergence of BP.
- \[2:15\]: Started notes on spiked matrix models.
- \[1:03\]: Watched video of lecture on AMP. Next step is to understand derivation of state evolution as well as relation to interpretation in terms of TAP equations.

## 8/16/19

- \[0:45\]: Finished video from yesterday.
- \[0:15\]: Skimmed Mezard-Montanari sections on REM.

## 8/16/19

- \[1:55\]: Wrote notes on Sidford lecture notes.
- \[1:25\]: Continued reading survey. Leaving it for now, maybe return in future. Makes more sense to got directly to recent papers.
- \[0:25\]: Watched part of video of talk by Montanari.

Had errand in morning.
## 8/15/19

- \[0:45\]: Watched part of a video of talk on non-convex optimization.
- \[2:30\]: Completed third section of survey. Don't quite understand replica symmetry.
 - \[1:20\]: Read first two sections of Sidford lecture notes.


Had duty in morning.

## 8/14/19

- \[1:45\]: Finished up videos on convex optimization. Next step is to carefully revise notes and understand smoothness.
- \[2:37\]: Continued with survey, completed second section.

Had duty in morning.

## 8/13/19

- \[2:45\]: Finished up strong convexity and cutting plane methods. Next step is to understand smoothness carefully.
- \[0:40\]: Read first section of a survey.
- \[0:50\]: Watched video lecture on TAP free energy.

Had long errand in afternoon.

## 8/12/19

- \[3:00\]: Wrote notes for second and third video lectures. Completed fourth lecture.
- \[1:15\]: Did planning for current research directions to pursue.

Woke up late again.

## 8/11/19

- \[3:00\]: Watched second, third, and half of fourth lecture on convex optimization.
- \[0:27\]: Thought about high level research directions.

Woke up late again.

## 8/10/19

- \[1:20\]: Took notes up to Section 2.5 of Mezard-Montanari; read the reset of Section 2.
- \[0:30\]: Watched introduction of video lecture about Ising Perceptron model.
- \[1:35\]: Reviewed projected gradient descent.
- \[0:50\]: Watched first lecture on convex optimization.

Woke up late again.
## 8/09/19

- \[1:40\]: Looked over first few sections of monograph on non-convex optimization and started taking notes. Should be a fairly straightforward read.
- \[1:55\]: Finished writing notes on computational-to-statistical gaps and TAP free energy. Plan to look into Montanari paper in the future.
- \[0:10\]: Skimmed Section 2 in Mezard-Montanari book.

Woke up late and had errands to do this morning.
## 8/08/19

- \[4:42\]: Read notes on computational-to-statistical gaps and TAP free energy. Things starting to make slightly more sense now.

## 8/07/19

- \[0:20\]: Watched part of a talk on MCMC.
- \[0:40\]: Read up to Section 2.5 in Montanari-Mezard book.
- \[1:25\]: Watched a video of a [talk](https://www.youtube.com/watch?v=SYP9ZpJmImw). Hoping to learn more about Barvinok's method for approximating partition functions.
- \[1:30\]: Tried to read about RS formulas for Bayesian inference problems. Got stuck due to lack of background; plan to read [notes](https://arxiv.org/pdf/1803.11132.pdf) first.

Woke up very late this morning. Wasn't as effective as the last two days. Tried to follow some video lectures, but got lost quickly. Main difference was that didn't have a single, big, concrete task. In future, will try to have one big task each day and just focus on that.


## 8/06/19

- \[0:35\]: Worked on draft, basically done.
- \[3:52\]: Review notes on Section 3 of Wainwright-Jordan survey. Read Bethe approximation in Section 4 and connection to BP. Read mean-field approximation in Section 5 and interpretation as KL-divergence minimization.
- \[0:25\]: Skimmed a blog post about sampling from multi-modal distributions. Hope to learn more about sampling in the future.

Had an errand this morning and woke up late, so delayed slightly.

## 8/05/19

- \[0:15\]: Updated logs after a few days.
- \[4:46\]: Read Section 3 of Wainwright-Jordan survey and reviewed [exponential families](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf). Struggled in the beginning due to notation and such, but realized at the end that the ideas are actually quite simple. In future, reading on this topic will become easier and more efficient. As always, first time beginning a new subject is hard. Need to carry momentum forward.

Was much more effective today. Did not take any breaks until all work finished. Did not obsess about working a particular number of hours; just recorded it in a detached manner. Ate a quick lunch and immediately got back to to work. Repeating this day 100 times will produce lots of progress.

## 8/04/19

- \[3:00\]: Read survey on BP. Watched talk on computing partition functions by convex programming hierarchies.
- \[0:15\]: Worked on draft.

Past three days were absolutely horrible; couldn't focus at all and didn't get much done. Need to stop optimizing for time and just focus on learning.

Had an idea for thesis subject.

Listened to ideas by [Firas Zahabi](https://www.youtube.com/watch?v=_fbCcWyYthQ). Read an [article](https://www.quantamagazine.org/graduate-student-solves-quantum-verification-problem-20181008/). Need to work *smarter* not harder. Work in manner that is sustainable and generates large *volume* over long period of time.

> Throw a bucket of water on a rock and has no effect. Let a drop of water fall onto a rock everyday and it creates a hole in that rock."

Only need to do things each day:

- Single drop of water
- Make sure the drop falls on the same spot it did yesterday
- Trust that the hole will form

## 8/03/19

- \[3:00\]: Read survey on BP and other related things.
- \[0:15\]: Worked on draft

## 8/02/19

- \[3:00\]: Didn't record this day until 08/05/19, so forgot details.

Woke up 1 hour late. Got delayed slightly by an errand. Had an interesting thought about interpolation method while in shower.

## 8/01/19

- \[1:05\]: Still reading Section 8 of Raginsky notes. Reached up to neural networks, skipped AdaBoost.
- \[1:15\]: Finished empirical process theory notes, except for some calculations in proof of Sudakov-Fernique. Roughly understand the main idea behind interpolation method.
- \[0:50\]: Returned to Talagrand book, skimmed over Section 1.3, understand the high-level approach now. Concluded that it would be best to return to Talagrand book later. In long term future, Mezard-Montanari and Moore-Mertens books might be useful. Next steps are to read Moore's survey on community detection and then return to Section 4 of El Alaoui thesis.
- \[1:50\]: Did one more pass over draft, made some minor edits, fixed some small calculations.


Ironically, encountered all these resources a year ago and now coming back to bite.
